{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Практическое задание\n",
    "#\n",
    "# Создать объект DataFrame из текста с частотами похожих слов\n",
    "#\n",
    "# Добавленное условие -- исключить числа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = 'text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_file(filepath=FILEPATH, encod=\"cp1251\"):\n",
    "    \"\"\" Read text from file \"\"\"\n",
    "    # для действительно больших текстов такой спопоб не подходит\n",
    "    with open(filepath or FILEPATH, \"r\", encoding=encod) as file_obj:\n",
    "        return file_obj.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    \"Calculates the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "    if n > m:\n",
    "        # Make sure n <= m, to use O(min(n,m)) space\n",
    "        a, b = b, a\n",
    "        n, m = m, n\n",
    "    i, j = 0, 0\n",
    "    current_row = range(n+1) # Keep current and previous row, not entire matrix\n",
    "    for i in range(1, m+1):\n",
    "        previous_row, current_row = current_row, [i]+[0]*n\n",
    "        for j in range(1, n+1):\n",
    "            add, delete, change = previous_row[j]+1, current_row[j-1]+1, previous_row[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change += 1\n",
    "            current_row[j] = min(add, delete, change)\n",
    "\n",
    "    return current_row[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(input_data):\n",
    "    \"\"\"prepare data for next work\"\"\"\n",
    "    \n",
    "    data_new = input_data[:]\n",
    "\n",
    "    #приведение к нижнему регистру и разбивка на слова\n",
    "    data_new = pd.DataFrame(re.split(r\"\\W+\", data_new.lower())[:-1], columns=[\"word\"])\n",
    "\n",
    "    # группировка по словам, одновременно формируется список уникальных слов\n",
    "    data_new[\"cnt\"] = 1\n",
    "    data_new = data_new.groupby([\"word\"], as_index=True).sum()\n",
    "\n",
    "    # удаление чисел\n",
    "    pattern = re.compile(r\"^\\d+$\")\n",
    "    idx = (el for el in data_new.index if not pattern.match(el))\n",
    "    data_new = data_new.loc[idx]\n",
    "    \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(input_data):\n",
    "    \"\"\"Create dictionary with similar words\"\"\"\n",
    "    # возвращает словарь с похожими словами и объект Serias с ключевым словом для каждого слова в индексе\n",
    "    data_new = input_data.copy()\n",
    "    data_new[\"is_selected\"] = \"\"\n",
    "    #  похожие слова ищем среди слов с 3-мя и более буквами\n",
    "    #  среди 1 и 2-х буквенных слов похожих не ожидается\n",
    "    idx = [el for el in data_new.index if len(el)>2]\n",
    "    # df -- объект Serias, индексы -- перечень слов с 3 и более буквами, \n",
    "    # значения -- \"\", если слово еще не включено уже в какую-либо группу похожих слов,\n",
    "    # или ключевое слово группы похожих слов\n",
    "    df = data_new[\"is_selected\"].loc[idx]\n",
    "    dict_synonym = {}\n",
    "\n",
    "    for i, idx_i in enumerate(df.index[:-1]):\n",
    "        if df[idx_i]:  # or i>100:   # для уменьшения времени работы для тестирования  \n",
    "            continue\n",
    "        else:\n",
    "            dict_synonym[idx_i] = [idx_i]  # еще не включенное слово становиться ключем для группы\n",
    "            df[idx_i] = idx_i\n",
    "            for idx_j in df.index[i+1:]:  # среди оставшихся слов, ищем слова, которые можно включить в группу\n",
    "                if not df[idx_j]:\n",
    "                    min_len = min(len(idx_i), len(idx_j))\n",
    "                    dist = distance(idx_i, idx_j)\n",
    "                    if (min_len<=5 and dist<=1) or (6<=min_len<=8 and dist<=2) or (9<=min_len and dist<=3):\n",
    "                        df[idx_j]=idx_i\n",
    "                        dict_synonym[idx_i].append(idx_j)\n",
    "    \n",
    "    # добавлено, так как последний элемент может быть пропущен в цикле\n",
    "    if not df.iloc[-1]: \n",
    "        dict_synonym[df.index[-1]] = [df.index[-1]]\n",
    "        df[df.index[-1]] = df.index[-1]\n",
    "    \n",
    "    # добавляем оставшиеся 1 и 2 буквенные слова\n",
    "    apd = data_new[\"is_selected\"].loc[set(data_new.index) - set(idx)]\n",
    "    apd[apd.index] = apd.index\n",
    "    df = df.append(apd)\n",
    "    dict_apd = apd.to_dict()\n",
    "    dict_apd = {el:[dict_apd[el]] for el in dict_apd}\n",
    "    dict_synonym.update(dict_apd)\n",
    "    \n",
    "    return dict_synonym, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = from_file()\n",
    "data = data_preparation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_synonym, df = create_dict(data)\n",
    "\n",
    "# Wall time: 5min 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_0 = data.cnt.sum()  # для последующей проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = data.copy()\n",
    "data_2[\"key_word\"] = df\n",
    "data_2 = data_2.groupby(data_2.key_word).sum()\n",
    "data_2 = data_2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lost words\n"
     ]
    }
   ],
   "source": [
    "words_1 = data_2.loc[\"cnt\"].sum()\n",
    "if words_0==words_1:\n",
    "    print(\"No lost words\")\n",
    "else:\n",
    "    print(\"Missed a few words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key_word  2е  30х30х30  3е  c  it  y  ya  z    а  аб    ...     ярослав  ясно  \\\n",
      "cnt        1         1   1  2   2  1   1  1  108   1    ...          11     1   \n",
      "\n",
      "key_word  і  іграе  ідэі  ў  ўлады  ўсе  ўсеагульнае  ўяўляюць  \n",
      "cnt       3      1     1  3      1    1            1         1  \n",
      "\n",
      "[1 rows x 2874 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(dict_synonym)\n",
    "print(data_2)\n",
    "#data_2.to_csv(\"result.csv\", encoding=\"cp1251\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
